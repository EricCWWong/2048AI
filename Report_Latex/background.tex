\section{Background}
\subsection{Game Rules}
The game is played on a 4x4 grid. Initially, there will be two tiles on the board with values "2" or "4".  The players will make moves "Left", "Right", "Up" and "Down". If the adjacent tile in the same direction of the move is the same, the two tiles will merge and form a new tile with values of the sum of the original value. Moves are not allowed if no tiles changes position after that invalid move. After each valid move, the a new tile of either "2" or "4" will be generated and randomly assigned to one of the empty tile. The goal of this game is to obtain a tile of value "2048" after a series of moves. When there are no longer valid moves available to the player, the game is terminated and the player loses the game.

\subsection{Reinforcement Learning}
Reinforcement learning is learning what actions to take in a certain situation  that will maximise our rewards. The learner is not told what actions to take, but instead must discover which actions yield the most reward by trying them \cite{sutton2018reinforcement}.  In our project, we will have 4 actions available for our agent, they are "Left", "Right", "Up" and "Down".  We will need a representation to present the current state of the board. This will be denoted as $\phi$ in the rest of the report. We have chosen the simplest approximation --- linear approximation --- where the q-value of the action  is related to the state linearly as shown below:
\begin{equation*}
q_{a} = \phi ^{T} w_{a}
\end{equation*}
In order to play the game better, we have to update the weight after each episode. In this project, multiple schemes such as Monte-carlo has been considered but we have eventually decided to stick with the q-learning scheme.
\\

Finally, after calculating the q-value, we choose to use epsilon-greedy policy to decide which action the agent takes. Furthermore,  in the second simulation, we have decided to adopt the decaying epsilon-greedy policy, where we allow the agent to explore in the beginning but slowly decreasing the freedom of exploration as we reach the later stages of the learning.
